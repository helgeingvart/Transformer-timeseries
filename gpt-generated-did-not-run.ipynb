{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-09T14:00:37.887878Z",
     "start_time": "2024-09-09T14:00:37.881724Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, n_classes):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        # Input projection from num_features to d_model\n",
    "        self.input_projection = nn.Linear(num_features, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 187, d_model))  # Assuming max seq_len of 1000\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Output MLP for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (batch_size, seq_len, num_features)\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Input projection (embedding)\n",
    "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Transpose for Transformer (needed by PyTorch's nn.Transformer)\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Pooling: Take the mean over the sequence dimension\n",
    "        x = x.mean(dim=0)  # (batch_size, d_model)\n",
    "\n",
    "        # Classification MLP\n",
    "        out = self.fc(x)  # (batch_size, n_classes)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "# Parameters\n",
    "num_features = 1     # Number of features in the time series\n",
    "seq_len = 187        # Length of each time series sequence\n",
    "d_model = 200        # Transformer model dimension\n",
    "nhead = 2            # Number of attention heads\n",
    "num_encoder_layers = 1 # Number of Transformer encoder layers\n",
    "dim_feedforward = 128 # Feedforward dimension\n",
    "dropout = 0.1        # Dropout rate\n",
    "n_classes = 5        # Number of output classes\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Datasets and dataloader classes for training, validation and testing\n",
    "I asked our friend Chad about this one too, giving him input with an excerpt from the training data file for information"
   ],
   "id": "bd5767be549c0afc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T13:58:26.847343Z",
     "start_time": "2024-09-09T13:58:25.275578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('data/mitbih_train.csv')\n",
    "df_test = pd.read_csv('data/mitbih_test.csv')\n",
    "\n",
    "# Custom Dataset class for time series data\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        # Add another dimension at the end if we later on wants to work on multivariate time-series\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.features = self.features.unsqueeze(-1)  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "#### TEST data #####\n",
    "# Separate features and labels\n",
    "labels = df_test.iloc[:, -1].values     # Last column as the label\n",
    "features = df_test.iloc[:, :-1].values  # All columns except the last one\n",
    "\n",
    "# Create Dataset and DataLoader for testing\n",
    "dataset = TimeSeriesDataset(features, labels)\n",
    "test_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "#### TRAIN/VALIDATION data ####\n",
    "# Separate features and labels\n",
    "labels = df_train.iloc[:, -1].values     # Last column as the label\n",
    "features = df_train.iloc[:, :-1].values  # All columns except the last one\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TimeSeriesDataset(features, labels)\n",
    "\n",
    "# Dataset splitting for training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx+1}:\")\n",
    "    print(f\"Inputs: {inputs.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n",
    "    break  # Just to show the first batch\n"
   ],
   "id": "971e4482d84dfa4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Inputs: torch.Size([100, 187, 1])\n",
      "Targets: torch.Size([100])\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Now, running through all epochs and batches"
   ],
   "id": "d6c2afe6e0fd3dd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:01:24.631816Z",
     "start_time": "2024-09-09T14:00:43.043989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = TimeSeriesTransformer(num_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, n_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Training and Validation Loop\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n"
   ],
   "id": "87cd27e9e2d0f949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Training Loss: 0.5519\n",
      "Validation Loss: 0.5008, Validation Accuracy: 0.8643\n",
      "Epoch 2/10\n",
      "Training Loss: 0.4321\n",
      "Validation Loss: 0.4027, Validation Accuracy: 0.8857\n",
      "Epoch 3/10\n",
      "Training Loss: 0.3912\n",
      "Validation Loss: 0.4058, Validation Accuracy: 0.8845\n",
      "Epoch 4/10\n",
      "Training Loss: 0.3680\n",
      "Validation Loss: 0.3766, Validation Accuracy: 0.8859\n",
      "Epoch 5/10\n",
      "Training Loss: 0.3501\n",
      "Validation Loss: 0.3501, Validation Accuracy: 0.8989\n",
      "Epoch 6/10\n",
      "Training Loss: 0.3314\n",
      "Validation Loss: 0.4011, Validation Accuracy: 0.8912\n",
      "Epoch 7/10\n",
      "Training Loss: 0.3094\n",
      "Validation Loss: 0.2940, Validation Accuracy: 0.9183\n",
      "Epoch 8/10\n",
      "Training Loss: 0.2857\n",
      "Validation Loss: 0.2655, Validation Accuracy: 0.9259\n",
      "Epoch 9/10\n",
      "Training Loss: 0.2589\n",
      "Validation Loss: 0.2436, Validation Accuracy: 0.9323\n",
      "Epoch 10/10\n",
      "Training Loss: 0.2493\n",
      "Validation Loss: 0.2409, Validation Accuracy: 0.9358\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
