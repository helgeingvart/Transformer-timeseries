{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/helgeingvart/Transformer-timeseries/blob/master/chad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ],
   "id": "51a34e012ec3760b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-19T19:20:50.507579Z",
     "start_time": "2024-09-19T19:20:50.503607Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, num_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, n_classes):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        # Input projection from num_features to d_model\n",
    "        self.input_projection = nn.Linear(num_features, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        # self.positional_encoding = nn.Parameter(torch.arange(seq_len).unsqueeze(0).unsqueeze(-1).float())\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, d_model))  # Assuming max seq_len of 1000\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Output MLP for classification\n",
    "        # self.fc = nn.Sequential(  \n",
    "        #               nn.Linear(d_model, 512), \n",
    "        #               nn.ReLU(), \n",
    "        #               nn.Linear(512, 256),\n",
    "        #               nn.ReLU(), \n",
    "        #               nn.Linear(256, 128), \n",
    "        #               nn.ReLU(), \n",
    "        #               nn.Linear(128, n_classes))\n",
    "        # \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (batch_size, seq_len, num_features)\n",
    "\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Input projection (embedding)\n",
    "        x = self.input_projection(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Add positional encoding. This code supports various lengths of the time-series (seq_len might vary)\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Transpose for Transformer (needed by PyTorch's nn.Transformer)\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Pooling: Take the mean over the sequence dimension\n",
    "        x = x.mean(dim=0)  # (batch_size, d_model)\n",
    "\n",
    "        # Classification MLP\n",
    "        out = self.fc(x)  # (batch_size, n_classes)\n",
    "\n",
    "        return out\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Datasets and dataloader classes for training, validation and testing\n",
    "I asked our friend Chad about this one too, giving him input with an excerpt from the training data file for information"
   ],
   "id": "bd5767be549c0afc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T17:09:42.986597Z",
     "start_time": "2024-09-19T17:09:41.198827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('data/mitbih_train.csv')\n",
    "df_test = pd.read_csv('data/mitbih_test.csv')\n",
    "\n",
    "# Custom Dataset class for time series data\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        # Add another dimension at the end if we later on wants to work on multivariate time-series\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.features = self.features.unsqueeze(-1)  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "#### TEST data #####\n",
    "# Separate features and labels\n",
    "labels = df_test.iloc[:, -1].values     # Last column as the label\n",
    "features = df_test.iloc[:, :-1].values  # All columns except the last one\n",
    "\n",
    "# Create Dataset and DataLoader for testing\n",
    "dataset = TimeSeriesDataset(features, labels)\n",
    "test_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "#### TRAIN/VALIDATION data ####\n",
    "# Separate features and labels\n",
    "labels = df_train.iloc[:, -1].values     # Last column as the label\n",
    "features = df_train.iloc[:, :-1].values  # All columns except the last one\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TimeSeriesDataset(features, labels)\n",
    "\n",
    "# Dataset splitting for training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx+1}:\")\n",
    "    print(f\"Inputs: {inputs.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n",
    "    break  # Just to show the first batch\n"
   ],
   "id": "971e4482d84dfa4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Inputs: torch.Size([100, 187, 1])\n",
      "Targets: torch.Size([100])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Now, running through all epochs and batches"
   ],
   "id": "d6c2afe6e0fd3dd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:24:17.682841Z",
     "start_time": "2024-09-19T19:20:59.188166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Parameters\n",
    "num_features = 1     # Number of features in the time series\n",
    "seq_len = 187        # Length of each time series sequence\n",
    "d_model = 200        # Transformer model dimension\n",
    "nhead = 2            # Number of attention heads\n",
    "num_encoder_layers = 1 # Number of Transformer encoder layers\n",
    "dim_feedforward = 128 # Feedforward dimension\n",
    "dropout = 0.1        # Dropout rate\n",
    "n_classes = 5        # Number of output classes\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = TimeSeriesTransformer(num_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, n_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Training and Validation Loop\n",
    "n_epochs = 50\n",
    "best_loss = 1e10 # Used for saving model upon optimal validation loss\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "        \n",
    "    val_accuracy = correct / total\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        print(\"saving best model\")\n",
    "        torch.save(model.state_dict(), 'myModel.pth')\n"
   ],
   "id": "87cd27e9e2d0f949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Training Loss: 0.5493\n",
      "Validation Loss: 0.4479, Validation Accuracy: 0.8713\n",
      "saving best model\n",
      "Epoch 2/50\n",
      "Training Loss: 0.3946\n",
      "Validation Loss: 0.3809, Validation Accuracy: 0.8859\n",
      "saving best model\n",
      "Epoch 3/50\n",
      "Training Loss: 0.3313\n",
      "Validation Loss: 0.3092, Validation Accuracy: 0.9047\n",
      "saving best model\n",
      "Epoch 4/50\n",
      "Training Loss: 0.2873\n",
      "Validation Loss: 0.3248, Validation Accuracy: 0.9065\n",
      "Epoch 5/50\n",
      "Training Loss: 0.2480\n",
      "Validation Loss: 0.2801, Validation Accuracy: 0.9224\n",
      "saving best model\n",
      "Epoch 6/50\n",
      "Training Loss: 0.2227\n",
      "Validation Loss: 0.2142, Validation Accuracy: 0.9401\n",
      "saving best model\n",
      "Epoch 7/50\n",
      "Training Loss: 0.2037\n",
      "Validation Loss: 0.1964, Validation Accuracy: 0.9487\n",
      "saving best model\n",
      "Epoch 8/50\n",
      "Training Loss: 0.1964\n",
      "Validation Loss: 0.2030, Validation Accuracy: 0.9450\n",
      "Epoch 9/50\n",
      "Training Loss: 0.1876\n",
      "Validation Loss: 0.1896, Validation Accuracy: 0.9454\n",
      "saving best model\n",
      "Epoch 10/50\n",
      "Training Loss: 0.1833\n",
      "Validation Loss: 0.1857, Validation Accuracy: 0.9481\n",
      "saving best model\n",
      "Epoch 11/50\n",
      "Training Loss: 0.1756\n",
      "Validation Loss: 0.1829, Validation Accuracy: 0.9494\n",
      "saving best model\n",
      "Epoch 12/50\n",
      "Training Loss: 0.1706\n",
      "Validation Loss: 0.1777, Validation Accuracy: 0.9513\n",
      "saving best model\n",
      "Epoch 13/50\n",
      "Training Loss: 0.1644\n",
      "Validation Loss: 0.1651, Validation Accuracy: 0.9557\n",
      "saving best model\n",
      "Epoch 14/50\n",
      "Training Loss: 0.1609\n",
      "Validation Loss: 0.1555, Validation Accuracy: 0.9580\n",
      "saving best model\n",
      "Epoch 15/50\n",
      "Training Loss: 0.1586\n",
      "Validation Loss: 0.1677, Validation Accuracy: 0.9529\n",
      "Epoch 16/50\n",
      "Training Loss: 0.1547\n",
      "Validation Loss: 0.1654, Validation Accuracy: 0.9564\n",
      "Epoch 17/50\n",
      "Training Loss: 0.1518\n",
      "Validation Loss: 0.1562, Validation Accuracy: 0.9592\n",
      "Epoch 18/50\n",
      "Training Loss: 0.1492\n",
      "Validation Loss: 0.1449, Validation Accuracy: 0.9604\n",
      "saving best model\n",
      "Epoch 19/50\n",
      "Training Loss: 0.1466\n",
      "Validation Loss: 0.1484, Validation Accuracy: 0.9600\n",
      "Epoch 20/50\n",
      "Training Loss: 0.1460\n",
      "Validation Loss: 0.1532, Validation Accuracy: 0.9576\n",
      "Epoch 21/50\n",
      "Training Loss: 0.1399\n",
      "Validation Loss: 0.1360, Validation Accuracy: 0.9639\n",
      "saving best model\n",
      "Epoch 22/50\n",
      "Training Loss: 0.1404\n",
      "Validation Loss: 0.1498, Validation Accuracy: 0.9591\n",
      "Epoch 23/50\n",
      "Training Loss: 0.1352\n",
      "Validation Loss: 0.1440, Validation Accuracy: 0.9604\n",
      "Epoch 24/50\n",
      "Training Loss: 0.1349\n",
      "Validation Loss: 0.1519, Validation Accuracy: 0.9595\n",
      "Epoch 25/50\n",
      "Training Loss: 0.1342\n",
      "Validation Loss: 0.1480, Validation Accuracy: 0.9585\n",
      "Epoch 26/50\n",
      "Training Loss: 0.1311\n",
      "Validation Loss: 0.1377, Validation Accuracy: 0.9626\n",
      "Epoch 27/50\n",
      "Training Loss: 0.1292\n",
      "Validation Loss: 0.1376, Validation Accuracy: 0.9613\n",
      "Epoch 28/50\n",
      "Training Loss: 0.1266\n",
      "Validation Loss: 0.1439, Validation Accuracy: 0.9599\n",
      "Epoch 29/50\n",
      "Training Loss: 0.1259\n",
      "Validation Loss: 0.1333, Validation Accuracy: 0.9634\n",
      "saving best model\n",
      "Epoch 30/50\n",
      "Training Loss: 0.1237\n",
      "Validation Loss: 0.1351, Validation Accuracy: 0.9625\n",
      "Epoch 31/50\n",
      "Training Loss: 0.1230\n",
      "Validation Loss: 0.1290, Validation Accuracy: 0.9652\n",
      "saving best model\n",
      "Epoch 32/50\n",
      "Training Loss: 0.1208\n",
      "Validation Loss: 0.1519, Validation Accuracy: 0.9553\n",
      "Epoch 33/50\n",
      "Training Loss: 0.1198\n",
      "Validation Loss: 0.1235, Validation Accuracy: 0.9661\n",
      "saving best model\n",
      "Epoch 34/50\n",
      "Training Loss: 0.1170\n",
      "Validation Loss: 0.1241, Validation Accuracy: 0.9654\n",
      "Epoch 35/50\n",
      "Training Loss: 0.1152\n",
      "Validation Loss: 0.1396, Validation Accuracy: 0.9626\n",
      "Epoch 36/50\n",
      "Training Loss: 0.1159\n",
      "Validation Loss: 0.1210, Validation Accuracy: 0.9683\n",
      "saving best model\n",
      "Epoch 37/50\n",
      "Training Loss: 0.1146\n",
      "Validation Loss: 0.1286, Validation Accuracy: 0.9656\n",
      "Epoch 38/50\n",
      "Training Loss: 0.1136\n",
      "Validation Loss: 0.1212, Validation Accuracy: 0.9678\n",
      "Epoch 39/50\n",
      "Training Loss: 0.1109\n",
      "Validation Loss: 0.1235, Validation Accuracy: 0.9672\n",
      "Epoch 40/50\n",
      "Training Loss: 0.1103\n",
      "Validation Loss: 0.1152, Validation Accuracy: 0.9681\n",
      "saving best model\n",
      "Epoch 41/50\n",
      "Training Loss: 0.1095\n",
      "Validation Loss: 0.1256, Validation Accuracy: 0.9660\n",
      "Epoch 42/50\n",
      "Training Loss: 0.1090\n",
      "Validation Loss: 0.1235, Validation Accuracy: 0.9688\n",
      "Epoch 43/50\n",
      "Training Loss: 0.1081\n",
      "Validation Loss: 0.1175, Validation Accuracy: 0.9693\n",
      "Epoch 44/50\n",
      "Training Loss: 0.1067\n",
      "Validation Loss: 0.1131, Validation Accuracy: 0.9707\n",
      "saving best model\n",
      "Epoch 45/50\n",
      "Training Loss: 0.1055\n",
      "Validation Loss: 0.1170, Validation Accuracy: 0.9668\n",
      "Epoch 46/50\n",
      "Training Loss: 0.1059\n",
      "Validation Loss: 0.1105, Validation Accuracy: 0.9713\n",
      "saving best model\n",
      "Epoch 47/50\n",
      "Training Loss: 0.1050\n",
      "Validation Loss: 0.1098, Validation Accuracy: 0.9698\n",
      "saving best model\n",
      "Epoch 48/50\n",
      "Training Loss: 0.1008\n",
      "Validation Loss: 0.1096, Validation Accuracy: 0.9724\n",
      "saving best model\n",
      "Epoch 49/50\n",
      "Training Loss: 0.1013\n",
      "Validation Loss: 0.1063, Validation Accuracy: 0.9726\n",
      "saving best model\n",
      "Epoch 50/50\n",
      "Training Loss: 0.1013\n",
      "Validation Loss: 0.1161, Validation Accuracy: 0.9693\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing\n",
    "The testing should be done on the model with the least validation loss, which was saved during the training."
   ],
   "id": "3696d6f9b86741d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:24:29.079051Z",
     "start_time": "2024-09-19T19:24:28.369156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "model = TimeSeriesTransformer(num_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, n_classes)\n",
    "# Load the saved model parameters\n",
    "model.load_state_dict(torch.load('myModel.pth', weights_only=False))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Run inference\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Evaluate performance\n",
    "test_accuracy = accuracy_score(all_targets, all_predictions)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "report = classification_report(all_targets, all_predictions, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ],
   "id": "d8a8025c505d6d58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9714\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9736    0.9955    0.9844     18117\n",
      "           1     0.9631    0.5629    0.7106       556\n",
      "           2     0.9463    0.8881    0.9163      1448\n",
      "           3     0.8542    0.5062    0.6357       162\n",
      "           4     0.9754    0.9627    0.9690      1608\n",
      "\n",
      "    accuracy                         0.9714     21891\n",
      "   macro avg     0.9425    0.7831    0.8432     21891\n",
      "weighted avg     0.9708    0.9714    0.9692     21891\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18035    12    39     2    29]\n",
      " [  232   313     8     0     3]\n",
      " [  143     0  1286    12     7]\n",
      " [   63     0    17    82     0]\n",
      " [   51     0     9     0  1548]]\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
