{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-21T13:45:23.574122800Z",
     "start_time": "2023-11-21T13:45:23.546112700Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8204\\1827785043.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[1;31m# Initialize the model, loss function, and optimizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 93\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTransformerForecasting\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     94\u001B[0m \u001B[0mcriterion\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mMSELoss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8204\\1827785043.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, input_dim, output_dim, d_model, nhead, num_encoder_layers)\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mTransformerForecasting\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membedding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpositional_encoding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPositionalEncoding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0md_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     54\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransformer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0md_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnhead\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_encoder_layers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0md_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8204\\1827785043.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, d_model, max_len)\u001B[0m\n\u001B[0;32m     40\u001B[0m         \u001B[0mdiv_term\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m10000.0\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0md_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpositional_encoding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mParameter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_len\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpositional_encoding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mposition\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mdiv_term\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpositional_encoding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mposition\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mdiv_term\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a synthetic multivariate time series dataset\n",
    "def generate_multivariate_time_series(num_samples, input_dim, output_dim):\n",
    "    np.random.seed(42)\n",
    "    data = np.random.randn(num_samples, input_dim + output_dim)\n",
    "    return data\n",
    "\n",
    "# Define the dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, input_length, target_length):\n",
    "        self.data = data\n",
    "        self.input_length = input_length\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_length - self.target_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.data[idx:idx + self.input_length, :]\n",
    "        target_data = self.data[idx + self.input_length:idx + self.input_length + self.target_length, :]\n",
    "        return torch.FloatTensor(input_data), torch.FloatTensor(target_data)\n",
    "\n",
    "# Define the PositionalEncoding module\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        self.positional_encoding[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        self.positional_encoding[:, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:, :x.size(1)].detach()\n",
    "\n",
    "# Define the Transformer model with positional encoding\n",
    "class TransformerForecasting(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=64, nhead=2, num_encoder_layers=2):\n",
    "        super(TransformerForecasting, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers)\n",
    "        self.linear = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x.permute(1, 0, 2))\n",
    "        x = self.transformer(x)\n",
    "        x = self.linear(x[-1, :, :])  # Use the last transformer layer output for forecasting\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 5  # Number of input features\n",
    "output_dim = 1  # Number of output features (for univariate time series, set this to 1)\n",
    "input_length = 10  # Length of input sequence\n",
    "target_length = 1  # Length of output sequence\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "# Generate synthetic data\n",
    "data = generate_multivariate_time_series(1000, input_dim, output_dim)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data, test_data = data[:train_size], data[train_size:]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TimeSeriesDataset(train_data, input_length, target_length)\n",
    "test_dataset = TimeSeriesDataset(test_data, input_length, target_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerForecasting(input_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.permute(1, 0, 2))  # Permute to (sequence_length, batch_size, input_dim)\n",
    "        loss = criterion(outputs, targets[:, 0, :])  # Assume univariate output, adjust if needed\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_values = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs.permute(1, 0, 2))\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        true_values.append(targets[:, 0, :].cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "# Inverse transform the data\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "true_values = scaler.inverse_transform(true_values)\n",
    "\n",
    "# Calculate and print the mean squared error\n",
    "mse = mean_squared_error(true_values, predictions)\n",
    "print(f'Mean Squared Error on Test Data: {mse:.4f}')\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(true_values, label='True Values')\n",
    "plt.plot(predictions, label='Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
